{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRO0Rg_yLe2-"
   },
   "source": [
    "# Машинное обучение, РЭШ\n",
    "\n",
    "## Практическое задание 2. Exploratory Data Analysis и линейная регрессия\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 05.11.2022\n",
    "\n",
    "Дедлайн: 17:00MSK 17.11.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQo6OlFVLe3A"
   },
   "source": [
    "### О задании\n",
    "В этом задании мы попытаемся научиться анализировать данные и выделять из них полезные признаки. Мы также научимся пользоваться `seaborn` и `sklearn`, а заодно привыкнем к основным понятиям машинного обучения.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов. Проверяющий имеет право снизить оценку за неэффективную реализацию или неопрятные графики.\n",
    "\n",
    "**Обратите внимание**, что в каждом разделе домашнего задания есть оцениваниемые задачи и есть вопросы. Вопросы дополняют задачи и направлены на то, чтобы проинтерпретировать или обосновать происходящее. Код без интерпретации не имеет смысла, поэтому отвечать на вопросы обязательно — за отсутствие ответов мы будем снижать баллы за задачи. Если вы ответите на вопросы, но не напишете корректный код к соответствующим оцениваемым задачам, то баллы за такое выставлены не будут.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "### Формат сдачи\n",
    "Задания сдаются на my.nes в виде Jupyter-notebook файла.\n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "Оценка: xx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqWTqjXkLe3B"
   },
   "source": [
    "В этом ноутбуке используется библиотека `folium` для визуализации карт. Она работает в google colab!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P6GB_c2OLe3I",
    "scrolled": false
   },
   "source": [
    "import folium\n",
    "\n",
    "# m = folium.Map(location=(55.7522200, 37.6155600), zoom_start=10)\n",
    "# m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vGzH3IkLe3N"
   },
   "source": [
    "Если вы всё сделали правильно, то выше должна открыться карта Москвы."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "734jlAXULe3O"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# %matplotlib inline\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "def pprint(*args, **kwargs):\n",
    "    print(\">>\", *args, **kwargs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1FzFZFmLe3S"
   },
   "source": [
    "## Часть 0. Подготовка (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQbqs_5tLe3T"
   },
   "source": [
    "**Задание 1 (1 балл)**. Мы будем работать с данными из соревнования [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration/overview), в котором нужно было предсказать длительность поездки на такси. Скачайте обучающую выборку из этого соревнования и загрузите ее:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uMZCBrS7Le3U"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df = pd.read_csv(\"data/nyc-taxi-trip-duration/train.csv\")\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check on nans\n",
    "df.isna().any()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUTMnCfTLe3Z"
   },
   "source": [
    "Обратите внимание на колонки `pickup_datetime` и `dropoff_datetime`. `dropoff_datetime` был добавлена организаторами только в обучающую выборку, то есть использовать эту колонку нельзя, давайте удалим ее. В `pickup_datetime` записаны дата и время начала поездки. Чтобы с ней было удобно работать, давайте преобразуем даты в `datetime`-объекты"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3DdlxtlTLe3Z"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df = df.drop(columns=\"dropoff_datetime\")\n",
    "df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"], format=\"%Y-%m-%d %H:%M:%S\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AI3I4ZNLe3i"
   },
   "source": [
    "В колонке `trip_duration` записано целевое значение, которое мы хотим предсказывать. Давайте посмотрим на распределение таргета в обучающей выборке. Для этого нарисуйте его гистограмму:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_zFtfMqzLe3i"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(data_frame=df, x=\"trip_duration\", log_y=True)\n",
    "fig.update_layout(title_text=\"Distribution of trip duration (log scale)\", xaxis_title = \"Trip duration\", yaxis_title = \"Count\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pprint(\"an extremely heave tail for the target, a few obvious large outliers spotted!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3laRVtvLe3o"
   },
   "source": [
    "**Вопрос**: Что можно сказать о целевой переменной по гистограмме её значений?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBt4ZK2VLe3q"
   },
   "source": [
    "В соревновании в качестве метрики качества использовалось RMSLE:\n",
    "$$\\text{RMSLE}(X, y, a) = \\sqrt{\\frac{1}{\\ell}\\sum_{i=1}^{\\ell} \\big(\\log{(y_i + 1)} - \\log{(a(x_i) + 1)}\\big)^2}$$\n",
    "\n",
    "**Вопрос**: Как вы думаете, почему авторы соревнования выбрали именно RMSLE, а не RMSE?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pprint(\"as the rmsle metric is more robust with outliers, while rmse is too sensitive\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omcYqq52Le3r"
   },
   "source": [
    "На семинаре мы рассматривали несколько моделей линейной регрессии в `sklearn`, но каждая из них оптимизировала среднеквадратичную ошибку (MSE), а не RMSLE. Давайте проделаем следующий трюк: будем предсказывать не целевую переменную, а ее *логарифм*. Обозначим $\\hat{y}_i = \\log{(y_i + 1)}$ — модифицированный таргет, а $\\hat{a}(x_i)$ — предсказание модели, которая обучалась на $\\hat{y}_i$, то есть логарифм таргета. Чтобы предсказать исходное значение, мы можем просто взять экспоненту от нашего предсказания: $a(x_i) = \\exp(\\hat{a}(x_i)) - 1$.\n",
    "\n",
    "**Вопрос**: Покажите, что оптимизация RMSLE для модели $a$ эквивалентна оптимизации MSE для модели $\\hat{a}$.\n",
    "\n",
    "**Доказательство**: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "$$\\text{RMSLE}(X, y, a) = \\sqrt{\\frac{1}{\\ell}\\sum_{i=1}^{\\ell} \\big(\\log{(y_i + 1)} - \\log{(a(x_i) + 1)}\\big)^2}$$\n",
    "$$\\hat{y}_i = \\log{(y_i + 1)}$$\n",
    "$$\\hat{a}_i = \\log{(a(x_i) + 1)} $$\n",
    "$$\\Rightarrow \\text{RMSLE}(X, y, a) = \\sqrt{\\frac{1}{\\ell}\\sum_{i=1}^{\\ell} \\big(\\hat{y}_i - \\hat{a}_i\\big)^2} = \\text{RMSE}(X, \\hat{y}, \\hat{a}) $$\n",
    "\n",
    "\n",
    "Итак, мы смогли свести задачу оптимизации RMSLE к задаче оптимизации MSE, которую мы умеем решать! Кроме того, у логарифмирования таргета есть еще одно полезное свойство. Чтобы его увидеть, добавьте к нашей выборке колонку `log_trip_duration` (воспользуйтесь `np.log1p`) и нарисуйте гистограмму модифицированного таргета по обучающей выборке. Удалите колонку со старым таргетом."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gMOHeNmYLe3s"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df[\"log_trip_duration\"] = np.log1p(df[\"trip_duration\"])\n",
    "df = df.drop(columns=\"trip_duration\")\n",
    "\n",
    "fig = px.histogram(data_frame=df, x=\"log_trip_duration\")\n",
    "fig.update_layout(title_text=\"Distribution of log trip duration\", xaxis_title = \"Log of trip duration\", yaxis_title = \"Count\", bargap=0.1)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3qrEhCBLe3x"
   },
   "source": [
    "Чтобы иметь некоторую точку отсчета, давайте посчитаем значение метрики при наилучшем константном предсказании:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z7YBRjDGLe3x"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "# the best constant forecast is the mean\n",
    "best_const_forecast = np.full((len(df[\"log_trip_duration\"]), 1), df[\"log_trip_duration\"].mean())\n",
    "pprint(f\"rmse with the best constant forecast: {root_mean_squared_error(df[['log_trip_duration']], best_const_forecast):.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ky5t3hCLLe33"
   },
   "source": [
    "## Часть 1. Изучаем `pickup_datetime` (2 балла)\n",
    "\n",
    "**Задание 2 (0.25 баллов)**. Для начала давайте посмотрим, сколько всего было поездок в каждый из дней. Постройте график зависимости количества поездок от дня в году (например, можно воспользоваться `sns.countplot`):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0RPfcwSqLe33"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "fig = px.histogram(data_frame=df, x=df[\"pickup_datetime\"].dt.date)\n",
    "fig.update_layout(title_text=\"Distribution of trips during the year\", xaxis_title = \"Date\", yaxis_title = \"Count\", bargap=0.1)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkZZZZHYLe37"
   },
   "source": [
    "**Вопрос**: Вы, вероятно, заметили, что на графике есть 2 периода с аномально маленькими количествами поездок. Вычислите, в какие даты происходили эти скачки вниз и найдите информацию о том, что происходило в эти дни в Нью-Йорке.<br>\n",
    "**Answer**:\n",
    "* On January 23, 2016, New York saw a massive blizzard\n",
    "* On May 28, 2016, a tropical storm Bonnie brought heavy rain up to the northeast coast\n",
    "\n",
    "Нарисуйте графики зависимости количества поездок от дня недели и от часов в сутках (воспользуйтесь `sns.relplot`):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "beMmwh_ILe38"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "counts = df.groupby(\n",
    "    [df[\"pickup_datetime\"].dt.day_name().rename(\"weekday\"), df[\"pickup_datetime\"].dt.hour.rename(\"hour\")]\n",
    ").size().reset_index(name=\"count\")\n",
    "\n",
    "fig = px.scatter(data_frame=counts, x=\"weekday\", y=\"count\", color=\"hour\", color_continuous_scale=\"Viridis\",\n",
    "                 category_orders={\"weekday\": [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]}\n",
    "                 )\n",
    "fig.update_layout(title_text=\"Distribution of trips over weekday and hour\", xaxis_title = \"Weekday\", yaxis_title = \"Count\")\n",
    "fig.show()\n",
    "del counts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEDGU5fsLe4A"
   },
   "source": [
    "**Задание 3 (0.5 баллов)**. Нарисуйте на одном графике зависимости количества поездок от часа в сутках для разных месяцев (разные кривые, соответствующие разным месяцам, окрашивайте в разные цвета, воспользуйтесь `hue` в `sns.relplot`). Аналогично нарисуйте зависимости количества поездок от часа в сутках для разных дней недели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "# counts on hour and month\n",
    "counts = df.groupby(\n",
    "    [df[\"pickup_datetime\"].dt.month_name().rename(\"month\"), df[\"pickup_datetime\"].dt.hour.rename(\"hour\")]\n",
    ").size().reset_index(name=\"count\")\n",
    "fig1 = px.scatter(data_frame=counts, x=\"hour\", y=\"count\", color=\"month\")\n",
    "fig1.update_layout(title_text=\"Distribution of trips over hour and month\", xaxis_title = \"Hour\", yaxis_title = \"Count\", legend_title=\"Month\")\n",
    "fig1.show()\n",
    "del counts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# counts on hour and day of the week\n",
    "counts = df.groupby(\n",
    "    [df[\"pickup_datetime\"].dt.day_name().rename(\"weekday\"), df[\"pickup_datetime\"].dt.hour.rename(\"hour\")]\n",
    ").size().reset_index(name=\"count\")\n",
    "fig2 = px.scatter(data_frame=counts, x=\"hour\", y=\"count\", color=\"weekday\")\n",
    "fig2.update_layout(title_text=\"Distribution of trips over hour and weekday\", xaxis_title = \"Hour\", yaxis_title = \"Count\", legend_title=\"Weekday\")\n",
    "fig2.show()\n",
    "del counts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VlNQ8EdLe4G"
   },
   "source": [
    "**Вопрос**: Какие выводы можно сделать, основываясь на графиках выше? Выделяются ли какие-нибудь дни недели? Месяца? Время суток? С чем это связано?<br>\n",
    "**Answer**:\n",
    "1) Across all months and days of the week in the sample, the total number of trips follows a daily cycle: demand is lowest late at night, increases around 7–8 a.m. during the morning rush hour, peaks in the evening, and then gradually declines.\n",
    "2) Nighttime trips are dominated by weekends, while morning trips are more typical on weekdays.\n",
    "3) January shows significantly fewer trips compared to other months in the sample, possibly a post-holiday effect.\n",
    "4) On weekends, morning trip volumes are substantially lower than on other days of the week.\n",
    "\n",
    "**Задание 4 (0.5 баллов)**. Разбейте выборку на обучающую и тестовую в отношении 7:3. По обучающей выборке нарисуйте график зависимости среднего логарифма времени поездки от дня недели. Затем сделайте то же самое, но для часа в сутках и дня в году."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "17de5zfNLe4H"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=1998)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def add_to_subplot(to_add: go.Figure = None, subplot: go.Figure = None, row: int = None, col: int = None,\n",
    "                   color: str = None, opacity: float = None) -> None:\n",
    "    for trace in to_add.data:\n",
    "        if color is not None:\n",
    "            trace.marker.color = color\n",
    "        if opacity is not None:\n",
    "            trace.opacity = opacity\n",
    "        subplot.add_trace(trace, row=row, col=col)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=[\"Mean log trip duration on weekday\", \"Mean log trip duration on hour\", \"Mean log trip duration on date\"])\n",
    "\n",
    "# counts on weekday\n",
    "counts = df.groupby(df[\"pickup_datetime\"].dt.day_name().rename(\"weekday\")).agg(mean_log_trip_duration=(\"log_trip_duration\", \"mean\")).reset_index(drop=False)\n",
    "fig1 = px.scatter(data_frame=counts, x=\"weekday\", y=\"mean_log_trip_duration\", category_orders={\"weekday\": [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]})\n",
    "add_to_subplot(fig1, fig, row=1, col=1)\n",
    "del counts, fig1\n",
    "\n",
    "# counts on hour\n",
    "counts = df.groupby(df[\"pickup_datetime\"].dt.hour.rename(\"hour\")).agg(mean_log_trip_duration=(\"log_trip_duration\", \"mean\")).reset_index(drop=False)\n",
    "fig2 = px.line(data_frame=counts, x=\"hour\", y=\"mean_log_trip_duration\")\n",
    "add_to_subplot(fig2, fig, row=1, col=2)\n",
    "del counts, fig2\n",
    "\n",
    "# counts on date\n",
    "counts = df.groupby(df[\"pickup_datetime\"].dt.dayofyear.rename(\"date\")).agg(mean_log_trip_duration=(\"log_trip_duration\", \"mean\")).reset_index(drop=False)\n",
    "fig3 = px.line(data_frame=counts, x=\"date\", y=\"mean_log_trip_duration\")\n",
    "add_to_subplot(fig3, fig, row=1, col=3)\n",
    "del counts, fig3\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text=\"Mean of log trip duration\")\n",
    "fig.update_xaxes(title_text=\"Weekday\", categoryorder=\"array\", categoryarray=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"], row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Hour\", row=1, col=2)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqWsbPRPLe4L"
   },
   "source": [
    "**Вопрос**: Похожи ли графики зависимости таргета от дня недели и от часа в сутках на аналогичные графики для количества поездок? Почему? Что происходит со средним таргетом в те два аномальных периода, что мы видели выше? Почему так происходит? Наблюдаете ли вы какой-нибудь тренд на графике зависимости `log_trip_duration` от номера дня в году?<br>\n",
    "**Answer**:\n",
    "1) The target's dependence on the day of the week is similar in both cases: the curves are hump-shaped, with a peak on Thursday. The same pattern holds for the hourly dependence: after a nighttime decline until around 5 a.m., there is a sharp increase during the morning rush hour, stabilizing by 3 p.m. The similarity is explained by the fact that both targets reflect the same underlying phenomenon, measured in different units, either the number of trips or the logarithm of total trip distance. An increase in one naturally corresponds to an increase in the other.\n",
    "2) We see spikes around the date of blizzard (either low demand on trips, or long duration because of snow), and storm.\n",
    "3) Yes, an upward trend is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCkUHkFkLe4M"
   },
   "source": [
    "Добавьте следующие признаки на основе `pickup_datetime`:\n",
    "1. День недели\n",
    "2. Месяц\n",
    "3. Час\n",
    "4. Является ли период аномальным (два бинарных признака, соответствующие двум аномальным периодам)\n",
    "5. Номер дня в году"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df[\"weekday\"] = df[\"pickup_datetime\"].dt.day_name()\n",
    "df[\"month\"] = df[\"pickup_datetime\"].dt.month\n",
    "df[\"hour\"] = df[\"pickup_datetime\"].dt.hour\n",
    "df[\"day_year_number\"] = df[\"pickup_datetime\"].dt.dayofyear\n",
    "\n",
    "# masks on anomalies\n",
    "df[\"if_anomaly1\"]  = df[\"pickup_datetime\"].dt.date.astype(\"str\").isin([\"2016-01-23\", \"2016-01-24\", \"2016-01-25\", \"2016-01-26\"])\n",
    "df[\"if_anomaly2\"]  = df[\"pickup_datetime\"].dt.date.astype(\"str\").isin([\"2016-05-28\", \"2016-05-29\", \"2016-05-30\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add created variables to subsamples\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=1998)\n",
    "\n",
    "# fix categorical and numerical features\n",
    "categorical_f1 = [\"if_anomaly1\", \"if_anomaly2\", \"weekday\"]\n",
    "numerical_f1 = [\"month\", \"hour\", \"day_year_number\"]\n",
    "\n",
    "x_train1, y_train1 = train_df[categorical_f1 + numerical_f1], train_df[\"log_trip_duration\"]\n",
    "x_test1, y_test1 = test_df[categorical_f1 + numerical_f1], test_df[\"log_trip_duration\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rIpKJZTLe4R"
   },
   "source": [
    "Итак, мы уже создали некоторое количество признаков.\n",
    "\n",
    "**Вопрос**: Какие из признаков стоит рассматривать как категориальные, а какие - как численные? Почему?<br>\n",
    "**Answer**: Among categorical features are a dummy on anomaly day and name of the day. Numerical features are all the rest: month number, hour, day number in year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdpPFJSrLe4R"
   },
   "source": [
    "**Задание 5 (0.75 баллов)**. Обучите `Ridge`-регрессию с параметрами по умолчанию, закодировав все категориальные признаки с помощью `OneHotEncoder`. Численные признаки отмасштабируйте с помощью `StandardScaler`. Используйте только признаки, которые мы выделили в этой части задания."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class MyLinearRegressionClass():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            x_train: pd.DataFrame,\n",
    "            y_train: pd.Series,\n",
    "            x_test: pd.DataFrame,\n",
    "            y_test: pd.Series,\n",
    "            categorical_f: list,\n",
    "            numerical_f: list,\n",
    "            estimator=Ridge()\n",
    "    ):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.categorical_f = categorical_f\n",
    "        self.numerical_f = numerical_f\n",
    "        self.estimator = estimator\n",
    "        self.model = None\n",
    "\n",
    "    def gs_linear_regression(self):\n",
    "        # ohe of categorical (drop the first to deal with linear dependence) and scaling of numerical\n",
    "        column_transformer = ColumnTransformer([\n",
    "            (\"ohe\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), self.categorical_f),\n",
    "            (\"scaler\", StandardScaler(), self.numerical_f)\n",
    "        ])\n",
    "\n",
    "        # pack all steps into pipe\n",
    "        pipe = Pipeline([\n",
    "            (\"column_transformer\", column_transformer),\n",
    "            (\"estimator\", self.estimator)\n",
    "        ])\n",
    "\n",
    "        param_grid = {\"estimator__alpha\": list(np.logspace(-5, 5, num = 11))}\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=-1, cv=5, scoring=\"neg_root_mean_squared_error\")\n",
    "        grid_search.fit(self.x_train, self.y_train)\n",
    "\n",
    "        self.model = grid_search.best_estimator_\n",
    "\n",
    "        pprint(f\"best params: {grid_search.best_params_}\")\n",
    "        pprint(f\"best cv rmse: {-grid_search.best_score_:.4f}\")\n",
    "\n",
    "        train_pred = self.model.predict(self.x_train)\n",
    "        baseline_train_pred = np.full(len(self.y_train), self.y_train.mean())\n",
    "\n",
    "        test_pred = self.model.predict(self.x_test)\n",
    "        baseline_test_pred = np.full(len(self.y_test), self.y_train.mean())\n",
    "\n",
    "        pprint(f\"obtained rmse on train {root_mean_squared_error(self.y_train, train_pred):.4f} (vs on baseline predict {root_mean_squared_error(self.y_train, baseline_train_pred):.4f})\")\n",
    "        pprint(f\"obtained rmse on test {root_mean_squared_error(self.y_test, test_pred):.4f} (vs on baseline predict {root_mean_squared_error(self.y_test, baseline_test_pred):.4f})\")\n",
    "\n",
    "\n",
    "    def linear_regression_predict(self):\n",
    "        test_pred = self.model.predict(self.x_test)\n",
    "        pprint(f\"obtained rmse on test {root_mean_squared_error(self.y_test, test_pred):.4f}\")\n",
    "\n",
    "\n",
    "    def linear_regression(self, alpha: float = None) -> pd.Series:\n",
    "        # ohe of categorical (drop the first to deal with linear dependence) and scaling of numerical\n",
    "        column_transformer = ColumnTransformer([\n",
    "            (\"ohe\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), self.categorical_f),\n",
    "            (\"scaler\", StandardScaler(), self.numerical_f)\n",
    "        ])\n",
    "\n",
    "        # if alpha is given, set it in estimator (without mutating the passed object)\n",
    "        estimator = self.estimator\n",
    "        if alpha is not None:\n",
    "            estimator = clone(estimator).set_params(alpha=alpha)\n",
    "\n",
    "        # pack all steps into pipe\n",
    "        pipe = Pipeline([\n",
    "            (\"column_transformer\", column_transformer),\n",
    "            (\"estimator\", estimator)\n",
    "        ])\n",
    "\n",
    "        self.model = pipe.fit(self.x_train, self.y_train)\n",
    "\n",
    "        train_pred = self.model.predict(self.x_train)\n",
    "        baseline_train_pred = np.full(len(self.y_train), self.y_train.mean())\n",
    "\n",
    "        test_pred = self.model.predict(self.x_test)\n",
    "        baseline_test_pred = np.full(len(self.y_test), self.y_train.mean())\n",
    "\n",
    "        pprint(f\"obtained rmse on train {root_mean_squared_error(self.y_train, train_pred):.4f} (vs on baseline predict {root_mean_squared_error(self.y_train, baseline_train_pred):.4f})\")\n",
    "        pprint(f\"obtained rmse on test {root_mean_squared_error(self.y_test, test_pred):.4f} (vs on baseline predict {root_mean_squared_error(self.y_test, baseline_test_pred):.4f})\")\n",
    "\n",
    "         # return residuals on train\n",
    "        return self.y_train - train_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr1 = MyLinearRegressionClass(x_train=x_train1, y_train=y_train1, x_test=x_test1, y_test=y_test1, categorical_f=categorical_f1, numerical_f=numerical_f1, estimator=Ridge())\n",
    "lr1.linear_regression()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en-aAdfhLe4W"
   },
   "source": [
    ";## Часть 2. Изучаем координаты (3 балла)\n",
    "Мы уже очень хорошо изучили данные о времени начала поездки, давайте теперь посмотрим на информацию о координатах начала и конца поездки. Мы подготовили для вас функцию, которая на карте рисует точки начала или конца поездки. Примеры ее вызова вы найдете ниже. Обратите внимание, что в эту функцию мы передаем лишь небольшой кусочек данных, посколько иначе функция будет работать очень долго"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YbzTO2L5Le4X"
   },
   "source": [
    "def show_circles_on_map(data, latitude_column, longitude_column, color):\n",
    "    \"\"\"\n",
    "    The function draws map with circles on it.\n",
    "    The center of the map is the mean of coordinates passed in data.\n",
    "    \n",
    "    data: DataFrame that contains columns latitude_column and longitude_column\n",
    "    latitude_column: string, the name of column for latitude coordinates\n",
    "    longitude_column: string, the name of column for longitude coordinates\n",
    "    color: string, the color of circles to be drawn\n",
    "    \"\"\"\n",
    "\n",
    "    location = (data[latitude_column].mean(), data[longitude_column].mean())\n",
    "    m = folium.Map(location=location)\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        folium.Circle(\n",
    "            radius=100,\n",
    "            location=(row[latitude_column], row[longitude_column]),\n",
    "            color=color,\n",
    "            fill_color=color,\n",
    "            fill=True\n",
    "        ).add_to(m)\n",
    "\n",
    "    return m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "siVl8ZUZLe4b"
   },
   "source": "show_circles_on_map(df.sample(10000), \"pickup_latitude\", \"pickup_longitude\", \"blue\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "krfxDGlTLe4f"
   },
   "source": "show_circles_on_map(df.sample(10000), \"dropoff_latitude\", \"dropoff_longitude\", \"blue\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gxxMWzJLe4j"
   },
   "source": [
    "**Вопрос**: Какие две точки выделяются на карте?<br>\n",
    "**Answer**: two airports (JFK and EWR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikRsvQHULe4j"
   },
   "source": [
    "**Задание 6 (0.75 балл)**. Как мы все прекрасно помним, $t = s / v_{\\text{ср}}$, поэтому очевидно, что самым сильным признаком будет расстояние, которое необходимо проехать. Мы не можем посчитать точное расстояние, которое необходимо преодолеть такси, но мы можем его оценить, посчитав кратчайшее расстояние между точками начала и конца поездки. Чтобы корректно посчитать расстояние между двумя точками на Земле, можно использовать функцию `haversine`. Также можно воспользоваться кодом с первого семинара. Посчитайте кратчайшее расстояние для объектов и запишите его в колонку `haversine`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eUTOhLKzLe4k"
   },
   "source": [
    "from haversine import haversine\n",
    "\n",
    "def distance(x):\n",
    "    return haversine((x[\"pickup_latitude\"], x[\"pickup_longitude\"]), (x[\"dropoff_latitude\"], x[\"dropoff_longitude\"]))\n",
    "\n",
    "df[\"haversine\"] = df.apply(distance, axis = 1)\n",
    "df[\"haversine\"].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM3m6n_yLe4o"
   },
   "source": [
    "Так как мы предсказываем логарифм времени поездки и хотим, чтобы наши признаки были линейно зависимы с этой целевой переменной, нам нужно логарифмировать расстояние: $\\log t = \\log s - \\log{v_{\\text{ср}}}$. Запишите логарифм `haversine` в отдельную колонку:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-JF4DSJlLe4o"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df[\"log_haversine\"] = np.log1p(df[\"haversine\"])\n",
    "df[\"log_haversine\"].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukL3ydU3Le4v"
   },
   "source": [
    "Убедимся, что логарифм расстояния лучше коррелирует с нашим таргетом, чем просто расстояние:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G4yornQtLe4w"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df[[\"haversine\", \"log_haversine\", \"log_trip_duration\"]].corr()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9J0-jjxLe40"
   },
   "source": [
    "**Задание 7 (0.75 балла)**. Давайте изучим среднюю скорость движения такси. Посчитайте среднюю скорость для каждого объекта обучающей выборки, разделив `haversine` на `trip_duration`, и нарисуйте гистограмму ее распределения"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uKfwpOAxLe41"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df[\"mean_speed\"] = df[\"haversine\"] / np.exp(df[\"log_trip_duration\"])\n",
    "\n",
    "# add created variables to subsamples\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=1998)\n",
    "\n",
    "fig = px.histogram(train_df, x=\"mean_speed\", log_y=True)\n",
    "fig.update_layout(title=\"Distribution of mean speed\", xaxis_title=\"Mean speed\", yaxis_title=\"Count\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcsAAOaaLe45"
   },
   "source": [
    "Как можно видеть по гистограмме, для некоторых объектов у нас получились очень больше значения скоростей. Нарисуйте гистограмму по объектам, для которых значение скорости получилось разумным (например, можно не включать рассмотрение объекты, где скорость больше некоторой квантили):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6w3QVsoXLe46"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "fig = px.histogram(train_df, x=\"mean_speed\", range_x=[0, np.percentile(train_df[\"mean_speed\"], 99)])\n",
    "fig.update_layout(title=\"Distribution of mean speed\", xaxis_title=\"Mean speed\", yaxis_title=\"Count\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nivT7rcgLe4_"
   },
   "source": [
    "Для каждой пары (день недели, час суток) посчитайте медиану скоростей. Нарисуйте с помощью `sns.heatmap` график, где по осям будут дни недели и часы, а в качестве значения функции - медиана скорости"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m5cl3RUDLe5A"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "counts = df.groupby([\"weekday\", \"hour\"]).agg(median_speed=(\"mean_speed\", \"median\")).reset_index(drop=False)\n",
    "fig = px.imshow(\n",
    "    pd.pivot(counts, index=\"weekday\", columns=\"hour\", values=\"median_speed\").reindex(\n",
    "        [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\",\"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    ))\n",
    "fig.update_layout(title=\"Distribution of median speed over weekday and hour\", xaxis_title=\"Hour\", yaxis_title=\"Weekday\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftssOrv5Le5E"
   },
   "source": [
    "Не забудьте удалить колонку со значением скорости из данных!\n",
    "\n",
    "**Вопрос**: Почему значение скорости нельзя использовать во время обучения?<br>\n",
    "**Answer**: because for new data we won't have `log_trip_duration`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6mSwA7B5Le5E"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df = df.drop(columns=[\"mean_speed\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFc0Z7S8Le5I"
   },
   "source": [
    "**Вопрос**: Посмотрите внимательно на график и скажите, в какие моменты времени скорость минимальна; максимальна.<br>\n",
    "**Answer**: \n",
    "1) Speed is the lowest between 9 a.m. and 12 p.m. from Tuesday to Friday, and between 1 p.m. and 3 p.m. from Tuesday to Thursday.\n",
    "2) Speed reaches its maximum on Sunday at 4 a.m. and on Monday at 6 a.m.\n",
    "\n",
    "Создайте признаки \"поездка совершается в период пробок\" и \"поездка совершается в период свободных дорог\" (естественно, они не должен зависеть от скорости!):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y3dJ12SsLe5I"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df[\"high_traffic\"] = ((df[\"weekday\"].isin([\"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"])) & (df[\"hour\"].isin(range(9, 13))) |\n",
    "                 (df[\"weekday\"].isin([\"Tuesday\", \"Wednesday\", \"Thursday\"])) & (df[\"hour\"].isin(range(13, 16))))\n",
    "df[\"small_traffic\"] = ((df[\"weekday\"].isin([\"Sunday\"])) & (df[\"hour\"] == 4)) | (df[\"weekday\"].isin([\"Monday\"]) & (df[\"hour\"] == 6))\n",
    "df[[\"high_traffic\", \"small_traffic\"]].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-wggpXpLe5O"
   },
   "source": [
    "**Задание 8 (0.25 балла)**. Как уже было замечено выше, на карте выделяются две точки вдали от Манхэттена. Для каждой из них добавьте в выборку два признака: началась ли поездка в ней и закончилась ли она в ней."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check coordinates of jfk and ewr\n",
    "jfk_coordinates = [40.640864, -73.790531]\n",
    "ewr_coordinates = [40.690345, -74.175100]\n",
    "\n",
    "m = folium.Map(location=[sum(x) / 2 for x in zip(jfk_coordinates, ewr_coordinates)])\n",
    "for coord, color in [(jfk_coordinates, \"blue\"), (ewr_coordinates, \"red\")]:\n",
    "    folium.CircleMarker(location=coord, radius=15, color=color, fill=True).add_to(m)\n",
    "# m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9kVVDQZnLe5O"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "def if_close_airport(coordinates: tuple = None, coordinates_airport: tuple = None, radius: float = None):\n",
    "    return haversine(coordinates, coordinates_airport) < radius\n",
    "\n",
    "radius_km = 2.\n",
    "df[\"jfk_dropoff\"] = df.apply(lambda row: if_close_airport(coordinates=(row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]),\n",
    "                                                          coordinates_airport=tuple(jfk_coordinates), radius=radius_km), axis=1)\n",
    "df[\"jfk_pickup\"] = df.apply(lambda row: if_close_airport(coordinates=(row[\"pickup_latitude\"], row[\"pickup_longitude\"]),\n",
    "                                                          coordinates_airport=tuple(jfk_coordinates), radius=radius_km), axis=1)\n",
    "df[\"ewr_dropoff\"] = df.apply(lambda row: if_close_airport(coordinates=(row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]),\n",
    "                                                          coordinates_airport=tuple(ewr_coordinates), radius=radius_km), axis=1)\n",
    "df[\"ewr_pickup\"] = df.apply(lambda row: if_close_airport(coordinates=(row[\"pickup_latitude\"], row[\"pickup_longitude\"]),\n",
    "                                                          coordinates_airport=tuple(ewr_coordinates), radius=radius_km), axis=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye6EouZzLe5V"
   },
   "source": [
    "Для каждого из созданных признаков нарисуйте \"ящик с усами\" (`sns.boxplot`) распределения логарифма времени поездки"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P5QXXlUtLe5W"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Boxplot of log trip duration when high traffic\", \"Boxplot of log trip duration when small traffic\"])\n",
    "\n",
    "fig1 = px.box(data_frame=df, x=\"high_traffic\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig1, fig, row=1, col=1)\n",
    "\n",
    "fig2 = px.box(data_frame=df, x=\"small_traffic\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig2, fig, row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Log trip duration\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Boxplot of log trip duration when JFK pickup\", \"Boxplot of log trip duration when JFK dropoff\"])\n",
    "\n",
    "fig1 = px.box(data_frame=df, x=\"jfk_pickup\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig1, fig, row=1, col=1)\n",
    "\n",
    "fig2 = px.box(data_frame=df, x=\"jfk_dropoff\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig2, fig, row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Log trip duration\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Boxplot of log trip duration when EWR pickup\", \"Boxplot of log trip duration when EWR dropoff\"])\n",
    "\n",
    "fig3 = px.box(data_frame=df, x=\"ewr_pickup\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig3, fig, row=1, col=1)\n",
    "\n",
    "fig4 = px.box(data_frame=df, x=\"ewr_dropoff\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig4, fig, row=1, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Log trip duration\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MOP_jEhLe5Z"
   },
   "source": [
    "**Вопрос**: судя по графикам, как вы думаете, хорошими ли получились эти признаки?<br>\n",
    "**Answer**: yes, even though they look close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YJQnuNwLe5a"
   },
   "source": [
    "<img src=\"https://www.dropbox.com/s/xson9nukz5hba7c/map.png?raw=1\" align=\"right\" width=\"20%\" style=\"margin-left: 20px; margin-bottom: 20px\">\n",
    "\n",
    "**Задание 9 (1 балл)**. Сейчас мы почти что не используем сами значения координат. На это есть несколько причин: по отдельности рассматривать широту и долготу не имеет особого смысла, стоит рассматривать их вместе. Во-вторых, понятно, что зависимость между нашим таргетом и координатами не линейная. Чтобы как-то использовать координаты, можно прибегнуть к следующему трюку: обрамим область с наибольшим количеством поездок прямоугольником (как на рисунке). Разобьем этот прямоугольник на ячейки. Каждой точке сопоставим номер ее ячейки, а тем точкам, что не попали ни в одну из ячеек, сопоставим значение -1.\n",
    "\n",
    "Напишите трансформер, который сначала разбивает показанную на рисунке область на ячейки, а затем создает два признака: номер ячейки, в которой началась поездка, и номер ячейки, в которой закончилась поездка. Количество строк и столбцов выберите самостоятельно.\n",
    "\n",
    "Обратите внимание, что все вычисления должны быть векторизованными, трансформер не должен модифицировать передаваемую ему выборку inplace, а все необходимые статистики (если они вдруг нужны) нужно считать только по обучающей выборке в методе `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fUqRhzhsLe5a"
   },
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MapGridTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_cols: int,\n",
    "            n_rows: int,\n",
    "            up_right_corner: tuple,\n",
    "            down_left_corner: tuple\n",
    "    ):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.up_right_corner = up_right_corner\n",
    "        self.down_left_corner = down_left_corner\n",
    "\n",
    "    def cell_definer(self, longitudes: pd.Series, latitudes: pd.Series):\n",
    "        # coordinates of borders\n",
    "        col_borders = np.linspace(self.down_left_corner[0], self.up_right_corner[0], self.n_cols + 1)\n",
    "        row_borders = np.linspace(self.down_left_corner[1], self.up_right_corner[1], self.n_rows + 1)\n",
    "\n",
    "        # get number of cell\n",
    "        index_x = np.searchsorted(col_borders, longitudes, \"right\")\n",
    "        index_y = np.searchsorted(row_borders, latitudes, \"right\")\n",
    "\n",
    "        # state of being inside the rectangle\n",
    "        inside_pos = (index_y < self.n_rows + 1) & (index_y > 0) & (index_x < self.n_cols + 1) & (index_x > 0)\n",
    "        # by default, all are out of rectangle, hence have -1\n",
    "        cells = np.full(longitudes.shape, -1)\n",
    "        cells[inside_pos] = (index_y[inside_pos] - 1) * self.n_cols + (index_x[inside_pos] - 1)\n",
    "\n",
    "        if np.unique(cells).shape[0] - 1 != self.n_cols * self.n_rows:\n",
    "           pprint(\"dimension error!\")\n",
    "        return cells\n",
    "\n",
    "    def transformer(self, df: pd.DataFrame = None):\n",
    "        df[\"cell_pickup\"] = self.cell_definer(df[\"pickup_longitude\"], df[\"pickup_latitude\"])\n",
    "        df[\"cell_dropoff\"] = self.cell_definer(df[\"dropoff_longitude\"], df[\"dropoff_latitude\"])\n",
    "\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_mgd = MapGridTransformer(n_cols=5, n_rows=8, up_right_corner=(-73.931280, 40.794559), down_left_corner=(-74.023999, 40.701835))\n",
    "df = class_mgd.transformer(df=df)\n",
    "df[[\"cell_pickup\", \"cell_dropoff\"]].value_counts()\n",
    "\n",
    "# add created variables to subsamples\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=1998)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIBlaHiPLe5e"
   },
   "source": [
    "**Задание 10 (0.25 балла)**. Обучите `Ridge`-регрессию со стандартными параметрами на признаках, которые мы выделили к текущему моменту. Категориальные признаки закодируйте через one-hot-кодирование, числовые признаки отмасштабируйте."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y0ovvVlJLe5f"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "# fix categorical and numerical features\n",
    "categorical_f2 = [\"if_anomaly1\", \"if_anomaly2\", \"weekday\", \"high_traffic\", \"small_traffic\", \"jfk_pickup\", \"jfk_dropoff\", \"ewr_pickup\", \"ewr_dropoff\"]\n",
    "numerical_f2 = [\"month\", \"hour\", \"day_year_number\", \"log_haversine\", \"cell_pickup\", \"cell_dropoff\"]\n",
    "\n",
    "x_train2, y_train2 = train_df[categorical_f2 + numerical_f2], train_df[\"log_trip_duration\"]\n",
    "x_test2, y_test2 = test_df[categorical_f2 + numerical_f2], test_df[\"log_trip_duration\"]\n",
    "lr2 = MyLinearRegressionClass(x_train=x_train2, y_train=y_train2, x_test=x_test2, y_test=y_test2, categorical_f=categorical_f2, numerical_f=numerical_f2)\n",
    "lr2.linear_regression();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lr1.linear_regression();",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgPm5JN3Le5k"
   },
   "source": [
    "## Часть 3. Изучаем оставшиеся признаки (1 балл)\n",
    "\n",
    "**Задание 11 (0.75 баллов)**. У нас осталось еще 3 признака, которые мы не исследовали: `vendor_id`, `passenger_count` и `store_and_fwd_flag`.\n",
    "\n",
    "**Вопрос**: Подумайте, почему каждый из этих признаков может быть потенциально полезным.<br>\n",
    "**Answer**:\n",
    "1) `vendor_id`– an indicator identifying the carrier that operated the trip. It provides information on the relative popularity of carriers and their overall market share.\n",
    "2) `passenger_count` – the number of passengers in the vehicle. It reflects overall customer volume per trip.\n",
    "3) `store_and_fwd_flag` – a flag indicating whether the trip record was temporarily stored in the vehicle's memory before being transmitted to the carrier due to a lack of server connection (Y = stored and forwarded; N = transmitted immediately). It captures potential delays in data transmission.\n",
    "\n",
    "Посчитайте, сколько есть уникальных значений у каждого из этих признаков:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0a84J8irLe5l"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df[[\"vendor_id\", \"passenger_count\", \"store_and_fwd_flag\"]].nunique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsK8NXPTLe5p"
   },
   "source": [
    "Постройте \"ящики с усами\" распределений логарифма времени поездки в зависимости от значений каждого из признаков"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-kVjpBX5Le5r"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=[\"Boxplot of log trip duration on vendor_id\", \"Boxplot of log trip duration on passenger count\", \"Boxplot of log trip duration on store flag\"])\n",
    "\n",
    "fig1 = px.box(data_frame=df, x=\"vendor_id\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig1, fig, row=1, col=1)\n",
    "\n",
    "fig2 = px.box(data_frame=df, x=\"passenger_count\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig2, fig, row=1, col=2)\n",
    "\n",
    "fig3 = px.box(data_frame=df, x=\"store_and_fwd_flag\", y=\"log_trip_duration\")\n",
    "add_to_subplot(fig3, fig, row=1, col=3)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Log trip duration\")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7HL3CFOLe5z"
   },
   "source": [
    "Переведите признаки `vendor_id` и `store_and_fwd_flag` в значения $\\{0;1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XgG-dLSrLe50"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df[\"vendor_id\"] = df[\"vendor_id\"].map({1: 0, 2: 1})\n",
    "df[\"store_and_fwd_flag\"] = df[\"store_and_fwd_flag\"].map({\"N\": 0, \"Y\": 1})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7M_JKOeLe53"
   },
   "source": [
    "**Вопрос**: Основываясь на графиках выше, как вы думаете, будут ли эти признаки сильными? <br>\n",
    "**Answer**: Nope, these features are rather poor. As the distributions for `vendor_id` are nearly identical across carriers. Similarly, the distributions for `store_and_fwd_flag` show only minor differences. In contrast, for `passenger_count`, the distributions differ noticeably only in the cases of maximum occupancy and zero passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfYLwhc_Le54"
   },
   "source": [
    "**Задание 12 (0.25 баллов)**. Проверьте свои предположения, обучив модель в том числе и на этих трех признаках. Обучайте `Ridge`-регрессию со стандартными параметрами. Категориальные признаки закодируйте one-hot-кодированием, а численные отмасштабируйте."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wVwCtcpKLe54"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "# add created variables to subsamples\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=1998)\n",
    "\n",
    "# fix categorical and numerical features\n",
    "categorical_f3 = [\"if_anomaly1\", \"if_anomaly2\", \"weekday\", \"high_traffic\", \"small_traffic\", \"jfk_pickup\", \"jfk_dropoff\", \"ewr_pickup\", \"ewr_dropoff\", \"vendor_id\", \"store_and_fwd_flag\", \"passenger_count\"]\n",
    "numerical_f3 = [\n",
    "    \"month\", \"hour\", \"day_year_number\", \"log_haversine\", \"cell_pickup\", \"cell_dropoff\"]\n",
    "\n",
    "x_train3, y_train3 = train_df[categorical_f3 + numerical_f3], train_df[\"log_trip_duration\"]\n",
    "x_test3, y_test3 = test_df[categorical_f3 + numerical_f3], test_df[\"log_trip_duration\"]\n",
    "lr3 = MyLinearRegressionClass(x_train=x_train3, y_train=y_train3, x_test=x_test3, y_test=y_test3, categorical_f=categorical_f3, numerical_f=numerical_f3)\n",
    "res3 = lr3.linear_regression()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lr2.linear_regression();",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7fcm9E7Le5_"
   },
   "source": [
    "Если признаки не дали какого-то ощутимого улучшения метрики, их можно выбросить из данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAh0M-TnLe6B"
   },
   "source": [
    "## Часть 4. Улучшаем модель (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtSacTLVLe6F"
   },
   "source": [
    "**Задание 13 (1 балл)**. В наших данных есть нетипичные объекты: с аномально маленьким времени поездки, с очень большим пройденным расстоянием или очень большими остатками регрессии. В этом задании предлагается исключить такие объекты из обучающей выборки. Для этого нарисуйте гистограммы распределения упомянутых выше величин, выберите объекты, которые можно назвать выбросами, и очистите обучающую выборку от них.\n",
    "\n",
    "Отметим, что хотя эти объекты и выглядят как выбросы, в тестовой выборке тоже скорее всего будут объекты с такими же странными значениями целевой переменной и/или признаков. Поэтому, возможно, чистка обучающей выборки приведёт к ухудшению качества на тесте. Тем не менее, всё равно лучше удалять выбросы из обучения, чтобы модель получалась более разумной и интерпретируемой."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6lN0zDxlLe6H"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "fig = make_subplots(rows=1, cols=3)\n",
    "\n",
    "add_to_subplot(px.histogram(res3), fig, row=1, col=1, color=\"red\")\n",
    "train_df = train_df.drop(index=res3[(res3 > 2.) | (res3 < -2.)].index)\n",
    "add_to_subplot(px.histogram(res3[(res3 < 2.) & (res3 > -2.)]), fig, row=1, col=1, color=\"blue\")\n",
    "\n",
    "add_to_subplot(px.histogram(data_frame=train_df, x=\"log_trip_duration\"), fig, row=1, col=2, color=\"red\")\n",
    "train_df = train_df[(train_df[\"log_trip_duration\"] > 4.) & (train_df[\"log_trip_duration\"] < 9.)]\n",
    "add_to_subplot(px.histogram(data_frame=train_df, x=\"log_trip_duration\"), fig, row=1, col=2, color=\"blue\")\n",
    "\n",
    "add_to_subplot(px.histogram(data_frame=train_df, x=\"log_haversine\"), fig, row=1, col=3, color=\"red\")\n",
    "train_df = train_df[train_df[\"log_haversine\"] < 3.5]\n",
    "add_to_subplot(px.histogram(data_frame=train_df, x=\"log_haversine\"), fig, row=1, col=3, color=\"blue\")\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "911FVFYyLe6K"
   },
   "source": [
    "Сейчас у нас очень много категориальных признаков. В категориальных признаках могут содержаться редкие категории, обычно это плохо: модель сильно переобучается на таких примерах. Попробуйте объединить редкие категории в одну. Естественно, делать это нужно только для действительно редких категорий."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GGMYoG5YLe6L"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "for cat in categorical_f3:\n",
    "    pprint(cat)\n",
    "    for value, ratio in train_df[cat].value_counts(normalize=True).items():\n",
    "        pprint(f\"{value} {ratio:.6f}\")\n",
    "    print(\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# let's put rare counts of passenger in one value\n",
    "train_df[\"passenger_count\"] = np.where(~train_df[\"passenger_count\"].isin([1, 2]), \"0or3+\", train_df[\"passenger_count\"])\n",
    "test_df[\"passenger_count\"] = np.where(~test_df[\"passenger_count\"].isin([1, 2]), \"0or3+\", test_df[\"passenger_count\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df[\"passenger_count\"].value_counts()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWLBw3B2Le6S"
   },
   "source": [
    "Обучите модель на очищенных данных и посчитайте качество на тестовой выборке."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_train4, y_train4 = train_df[categorical_f3 + numerical_f3], train_df[\"log_trip_duration\"]\n",
    "x_test4, y_test4 = test_df[categorical_f3 + numerical_f3], test_df[\"log_trip_duration\"]\n",
    "\n",
    "lr4 = MyLinearRegressionClass(x_train=x_train4, y_train=y_train4, x_test=x_test4, y_test=y_test4, categorical_f=categorical_f3, numerical_f=numerical_f3)\n",
    "lr4.linear_regression();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lr3.linear_regression();",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf9RCMV5Le6T"
   },
   "source": [
    "**Задание 14 (1 балл)**. После OneHot-кодирования количество признаков в нашем датасете сильно возрастает. Посчитайте колиество признаков до и после кодирования категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AMWA1ONnLe6U"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "pprint(f\"before ohe: {len(categorical_f3)}\")\n",
    "pprint(f\"after ohe: {OneHotEncoder().fit_transform(X=x_train3[categorical_f3]).shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBrMYC2bLe6Y"
   },
   "source": [
    "Попробуйте обучить не `Ridge`-, а `Lasso`-регрессию. Какой метод лучше?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RajjatIMLe6Z"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "lr5 = MyLinearRegressionClass(x_train=x_train4, y_train=y_train4, x_test=x_test4, y_test=y_test4, categorical_f=categorical_f3, numerical_f=numerical_f3, estimator=Lasso())\n",
    "lr5.linear_regression();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lr4.linear_regression();",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTUpCqfnLe6c"
   },
   "source": [
    "Разбейте обучающую выборку на обучающую и валидационную в отношении 8:2. По валидационной выборке подберите оптимальные значения параметра регуляризации (по логарифмической сетке) для `Ridge` и `Lasso`, на тестовой выборке измерьте качество лучшей полученной модели."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HSo2m52zLe6d"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=1998)\n",
    "\n",
    "x_train5, y_train5 = train_df[categorical_f3 + numerical_f3], train_df[\"log_trip_duration\"]\n",
    "x_test5, y_test5 = val_df[categorical_f3 + numerical_f3], val_df[\"log_trip_duration\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr6 = MyLinearRegressionClass(x_train=x_train5, y_train=y_train5, x_test=x_test5, y_test=y_test5, categorical_f=categorical_f3, numerical_f=numerical_f3, estimator=Ridge())\n",
    "\n",
    "# for alpha in list(np.logspace(-7, 5, num = 11)):\n",
    "#     pprint(f\"current alpha: {alpha}\")\n",
    "#     lr6.linear_regression(alpha=alpha);\n",
    "\n",
    "# I want to try grid search instead\n",
    "lr6.linear_regression();\n",
    "lr6.gs_linear_regression();\n",
    "\n",
    "# assess error on test\n",
    "lr6.x_test = x_test4\n",
    "lr6.y_test = y_test4\n",
    "lr6.linear_regression_predict()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr7 = MyLinearRegressionClass(x_train=x_train5, y_train=y_train5, x_test=x_test5, y_test=y_test5, categorical_f=categorical_f3, numerical_f=numerical_f3, estimator=Lasso())\n",
    "\n",
    "for alpha in list(np.logspace(-7, 5, num = 11)):\n",
    "    pprint(f\"current alpha: {alpha}\")\n",
    "    lr7.linear_regression(alpha=alpha);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajotnL56Le6f"
   },
   "source": [
    "Для каждого перебранного `alpha` для Lasso посчитайте количество нулевых весов в модели и нарисуйте график зависимости его от `alpha`. Как сильно придётся потерять в качестве, если мы хотим с помощью Lasso избавиться хотя бы от половины признаков?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hPZfowyaLe6h"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7Fi7pmcLe6k"
   },
   "source": [
    "<img src=\"https://www.dropbox.com/s/wp4jj0599np17lh/map_direction.png?raw=1\" width=\"20%\" align=\"right\" style=\"margin-left: 20px\">\n",
    "\n",
    "**Задание 15 (1 балл)**. Часто бывает полезным использовать взаимодействия признаков (feature interactions), то есть строить новые признаки на основе уже существующих. Выше мы разбили карту Манхэттена на ячейки и придумали признаки \"из какой ячейки началась поездка\" и \"в какой ячейке закончилась поездка\".\n",
    "\n",
    "Давайте попробуем сделать следующее: посчитаем, сколько раз встречается каждая возможная пара этих признаков в нашем датасете и выберем 100 самых частых пар. Закодируем поездки с этими частыми парами как категориальный признак, остальным объектам припишем -1. Получается, что мы закодировали, откуда и куда должно было ехать такси.\n",
    "\n",
    "Также можете придумать ещё какой-нибудь способ сделать признаки про маршрут. Если эти признаки будут давать хороший прирост в качестве, то за это могут быть даны дополнительные бонусные баллы.\n",
    "\n",
    "**Вопрос**: Почему такой признак потенциально полезный? Почему линейная модель не может самостоятельно \"вытащить\" эту информацию, ведь у нее в распоряжении есть признаки \"из какой ячейки началась поездка\" и \"в какой ячейке закончилась поездка\"?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kOhOOKmjLe6l"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWqNCIuoLe6r"
   },
   "source": [
    "Заново обучите модель (`Ridge`, если она дала более высокое качество в предыдущих экспериментах, и `Lasso` иначе) на новых даннных и посчитайте качество на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vBSOyScILe6s"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ky5t3hCLLe33",
    "en-aAdfhLe4W",
    "FgPm5JN3Le5k"
   ],
   "name": "homework_practice_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
