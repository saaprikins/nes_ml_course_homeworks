{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csMaH5hpuVTz"
   },
   "source": [
    "# Машинное обучение, РЭШ\n",
    "\n",
    "## [Практическое задание 3. Градиентный спуск своими руками](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\n",
    "\n",
    "### Общая информация\n",
    "Дата выдачи: 17.11.2022\n",
    "\n",
    "Дедлайн: 23:59MSK 24.11.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRJFT5yPuVUC"
   },
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска.\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Задания загружаются на my.nes. Присылать необходимо ноутбук с выполненным заданием. \n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "**Оценка**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-HrP6yvuVUE"
   },
   "source": [
    "## Реализация градиентного спуска\n",
    "\n",
    "Реализуйте линейную регрессию с функцией потерь MSE, обучаемую с помощью:\n",
    "\n",
    "**Задание 1 (1 балл)** Градиентного спуска;\n",
    "\n",
    "**Задание 2.1 (2 балла)** Стохастического градиентного спуска + Batch SGD;\n",
    "\n",
    "**Задание 2.2 (2 балла)** SGD Momentum;\n",
    "\n",
    "**Бонусное задание (2 балл)** Adagrad, RMSProp, Adam;\n",
    "\n",
    "Во всех пунктах необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы;\n",
    "* Циклы средствами python допускается использовать только для итераций градиентного спуска;\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "\n",
    "    * проверку на евклидовую норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, задаваемого параметром `tolerance`);\n",
    "    * достижение максимального числа итераций (например, 10000, задаваемого параметром `max_iter`).\n",
    "* Чтобы проследить, что оптимизационный процесс действительно сходится, будем использовать атрибут класса `loss_history` — в нём после вызова метода `fit` должны содержаться значения функции потерь для всех итераций, начиная с первой (до совершения первого шага по антиградиенту);\n",
    "* Инициализировать веса можно случайным образом или нулевым вектором. \n",
    "\n",
    "\n",
    "Ниже приведён шаблон класса, который должен содержать код реализации каждого из методов."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:06:01.237931Z",
     "start_time": "2026-02-25T22:06:01.231558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pprint(*args, **kwargs):\n",
    "    print(\">>\", *args, **kwargs)"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vzu1xnsCuVUG",
    "ExecuteTime": {
     "end_time": "2026-02-25T22:24:29.501477Z",
     "start_time": "2026-02-25T22:24:29.477446Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self, gd_type='full', optimizer_type='full', loss_function=\"mse\",\n",
    "                 tolerance=1e-4, max_iter=1e5, w0=None, eta=1e-2, batch_size=1, alpha_momentum=0.4, eps_adagrad=1e-3, alpha_rmsprop=0.8):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.loss_function = loss_function\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = int(max_iter)\n",
    "        self.w0 = w0\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.batch_size = int(batch_size) # batch size for batch sgd\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.r2_history = None\n",
    "        self.w_history = None # list of weights at each training iteration\n",
    "        self.alpha_momentum = alpha_momentum\n",
    "        self.eps_adagrad = eps_adagrad\n",
    "        self.alpha_rmsprop = alpha_rmsprop\n",
    "        self.final_iterations = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        # initialize vector of weights\n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.zeros(X.shape[1])\n",
    "\n",
    "        self.w = self.w0.copy()\n",
    "        self.w_history = [self.w.copy()]\n",
    "        self.loss_history = [self.calc_loss(X, y)]\n",
    "        self.r2_history = [r2_score(y, self.predict(X))]\n",
    "\n",
    "        eta_t = self.eta\n",
    "        h_t_previous = np.zeros_like(self.w)\n",
    "\n",
    "        # parameters for adagrad\n",
    "        G_k = np.zeros_like(self.w)\n",
    "\n",
    "        for iteration in tqdm(range(self.max_iter)):\n",
    "\n",
    "            # manipulating step eta (for stochastic and stochastic batch gradient calculation)\n",
    "            # note: robbins-monro conditions\n",
    "            if self.gd_type != \"full\":\n",
    "                eta_t = self.eta / np.sqrt(iteration + 1)\n",
    "\n",
    "            # manipulating the decrease of anti-gradient\n",
    "            gradient = self.calc_gradient(X,y)\n",
    "            if self.optimizer_type == \"adagrad\":\n",
    "                G_k += gradient**2\n",
    "                h_t = self.eta * gradient / np.sqrt(G_k + self.eps_adagrad)\n",
    "            elif self.optimizer_type == \"rmsprop\":\n",
    "                G_k = self.alpha_rmsprop * G_k + (1 - self.alpha_rmsprop) * gradient**2\n",
    "                h_t = self.eta * gradient / np.sqrt(G_k + self.eps_adagrad)\n",
    "            elif self.optimizer_type == \"momentum\":\n",
    "                h_t = self.alpha_momentum * h_t_previous + eta_t * gradient\n",
    "                h_t_previous = h_t\n",
    "            elif self.optimizer_type == \"full\":\n",
    "                h_t = eta_t * gradient\n",
    "            else:\n",
    "                raise Exception(\"Unknown optimizer type\")\n",
    "\n",
    "            self.w -= h_t\n",
    "            self.w_history.append(self.w.copy())\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "            self.r2_history.append(r2_score(y, self.predict(X)))\n",
    "\n",
    "            self.final_iterations = iteration\n",
    "\n",
    "            if np.linalg.norm(self.w_history[-1] - self.w_history[-2]) <= self.tolerance:\n",
    "                break\n",
    "\n",
    "        # transform list to np.array\n",
    "        self.w_history = np.array(self.w_history)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        return X @ self.w\n",
    "\n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d) (ell can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        if self.gd_type == \"full\":\n",
    "            return 2 * X.T @ (X @ self.w - y) / y.shape[0]\n",
    "        elif self.gd_type == \"stochastic\":\n",
    "            sample = np.random.choice(X.shape[0], size=self.batch_size, replace=False)\n",
    "            return 2 * X[sample].T @ (X[sample] @ self.w - y[sample]) / self.batch_size\n",
    "        else:\n",
    "            raise Exception(\"Unknown gd_type!\")\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        if self.loss_function == \"mse\":\n",
    "            return mean_squared_error(y, self.predict(X))\n",
    "        else:\n",
    "            raise Exception(\"Unknown loss function!\")"
   ],
   "outputs": [],
   "execution_count": 119
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAGrUd74uVUX"
   },
   "source": [
    "**Задание 3 (0 баллов)**\n",
    "* Загрузите данные из домашнего задания 2 ([train.csv](https://www.kaggle.com/c/nyc-taxi-trip-duration/data));\n",
    "* Разбейте выборку на обучающую и тестовую в отношении 7:3 с random_seed=0;\n",
    "* Преобразуйте целевую переменную `trip_duration` как $\\hat{y} = \\log{(y + 1)}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:16:48.481587Z",
     "start_time": "2026-02-25T22:16:46.603911Z"
    }
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "df = pd.read_csv(\"data/nyc-taxi-trip-duration/train.csv\")"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:16:57.194546Z",
     "start_time": "2026-02-25T22:16:49.177871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mini eda\n",
    "df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "df = df.drop(columns=\"dropoff_datetime\")\n",
    "\n",
    "df[\"log_trip_duration\"] = np.log1p(df[\"trip_duration\"])\n",
    "df = df.drop(columns=\"trip_duration\")\n",
    "\n",
    "df[\"hour\"] = df[\"pickup_datetime\"].dt.hour\n",
    "df[\"month\"] = df[\"pickup_datetime\"].dt.month\n",
    "df[\"day_year_number\"] = df[\"pickup_datetime\"].dt.dayofyear\n",
    "\n",
    "from haversine import haversine\n",
    "def distance(x):\n",
    "    return haversine((x[\"pickup_latitude\"], x[\"pickup_longitude\"]), (x[\"dropoff_latitude\"], x[\"dropoff_longitude\"]))\n",
    "\n",
    "df[\"haversine\"] = df.apply(distance, axis = 1)\n",
    "df[\"log_haversine\"] = np.log1p(df[\"haversine\"])\n",
    "df = df.drop(columns=\"haversine\")"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:21:33.439194Z",
     "start_time": "2026-02-25T22:21:33.073886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=0)\n",
    "\n",
    "# fix numerical features\n",
    "numerical_f = [\"month\", \"hour\", \"day_year_number\", \"log_haversine\"]\n",
    "\n",
    "x_train, y_train = train_df[numerical_f], train_df[\"log_trip_duration\"].to_numpy()\n",
    "x_test, y_test = test_df[numerical_f], test_df[\"log_trip_duration\"].to_numpy()"
   ],
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:21:34.256724Z",
     "start_time": "2026-02-25T22:21:34.128688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from statsmodels.api import add_constant\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "column_transformer = ColumnTransformer([(\"numeric\", StandardScaler(), numerical_f)])\n",
    "pipe = Pipeline([(\"column_transformer\", column_transformer)])\n",
    "\n",
    "x_train = add_constant(pipe.fit_transform(x_train))\n",
    "x_test = add_constant(pipe.transform(x_test))"
   ],
   "outputs": [],
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaX55pBGuVUd"
   },
   "source": "**Задание 4 (3 балла)**. Обучите и провалидируйте модели на данных из предыдущего пункта, сравните качество между методами по метрикам MSE и $R^2$. Исследуйте влияние параметров `max_iter` и `eta` на процесс оптимизации. Согласуется ли оно с вашими ожиданиями?"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pMNYu96euVUe",
    "ExecuteTime": {
     "end_time": "2026-02-25T22:25:10.263862Z",
     "start_time": "2026-02-25T22:25:06.011727Z"
    }
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "temp_class = LinearReg(optimizer_type=\"full\", gd_type=\"full\", max_iter=1000) # full optimizer + full gradient\n",
    "temp_class.fit(X=x_train, y=y_train)\n",
    "pprint(f\"number of iterations: {temp_class.final_iterations}\")\n",
    "pprint(f\"loss function on test: {temp_class.calc_loss(X=x_test, y=y_test)}\")\n",
    "pprint(f\"r2 on test: {r2_score(y_test, temp_class.predict(x_test))}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 355/1000 [00:04<00:07, 83.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> number of iterations: 355\n",
      ">> loss function on test: 0.27209800353887953\n",
      ">> r2 on test: 0.567857119236314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:22:00.216890Z",
     "start_time": "2026-02-25T22:21:42.023781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp_class = LinearReg(optimizer_type=\"full\", gd_type=\"stochastic\", max_iter=1000) # stochastic on 1 element\n",
    "temp_class.fit(X=x_train, y=y_train)\n",
    "pprint(f\"number of iterations: {temp_class.final_iterations}\")\n",
    "pprint(f\"loss function on test: {temp_class.calc_loss(X=x_test, y=y_test)}\")\n",
    "pprint(f\"r2 on test: {r2_score(y_test, temp_class.predict(x_test))}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 55.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> number of iterations: 999\n",
      ">> loss function on test: 3.810164641382227\n",
      ">> r2 on test: -5.051259115819208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:31:11.702550Z",
     "start_time": "2026-02-25T22:30:32.280744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp_class = LinearReg(optimizer_type=\"full\", gd_type=\"stochastic\", batch_size=2e5, max_iter=1000) # batch sgd\n",
    "temp_class.fit(X=x_train, y=y_train)\n",
    "pprint(f\"number of iterations: {temp_class.final_iterations}\")\n",
    "pprint(f\"loss function on test: {temp_class.calc_loss(X=x_test, y=y_test)}\")\n",
    "pprint(f\"r2 on test: {r2_score(y_test, temp_class.predict(x_test))}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:39<00:00, 25.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> number of iterations: 999\n",
      ">> loss function on test: 3.8219807565110155\n",
      ">> r2 on test: -5.070025332273495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:31:42.471562Z",
     "start_time": "2026-02-25T22:31:35.000291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp_class = LinearReg(optimizer_type=\"momentum\", gd_type=\"stochastic\", batch_size=1e5, alpha_momentum=0.9, max_iter=1000) # momentum method\n",
    "temp_class.fit(X=x_train, y=y_train)\n",
    "pprint(f\"number of iterations: {temp_class.final_iterations}\")\n",
    "pprint(f\"loss function on test: {temp_class.calc_loss(X=x_test, y=y_test)}\")\n",
    "pprint(f\"r2 on test: {r2_score(y_test, temp_class.predict(x_test))}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 249/1000 [00:07<00:22, 33.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> number of iterations: 249\n",
      ">> loss function on test: 0.2721237015874943\n",
      ">> r2 on test: 0.5678163058947461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:34:12.658309Z",
     "start_time": "2026-02-25T22:33:43.015875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp_class = LinearReg(optimizer_type=\"adagrad\", gd_type=\"stochastic\", batch_size=1e5, eps_adagrad=0.5, max_iter=1000) # adagard method\n",
    "temp_class.fit(X=x_train, y=y_train)\n",
    "pprint(f\"number of iterations: {temp_class.final_iterations}\")\n",
    "pprint(f\"loss function on test: {temp_class.calc_loss(X=x_test, y=y_test)}\")\n",
    "pprint(f\"r2 on test: {r2_score(y_test, temp_class.predict(x_test))}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:29<00:00, 33.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> number of iterations: 999\n",
      ">> loss function on test: 34.6231641989315\n",
      ">> r2 on test: -53.98810620983649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-25T22:34:38.600286Z",
     "start_time": "2026-02-25T22:34:14.958064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp_class = LinearReg(optimizer_type=\"rmsprop\", gd_type=\"stochastic\", batch_size=1e5, eps_adagrad=0.5, alpha_rmsprop=0.9, max_iter=1000) # rmsprop method\n",
    "temp_class.fit(X=x_train, y=y_train)\n",
    "pprint(f\"number of iterations: {temp_class.final_iterations}\")\n",
    "pprint(f\"loss function on test: {temp_class.calc_loss(X=x_test, y=y_test)}\")\n",
    "pprint(f\"r2 on test: {r2_score(y_test, temp_class.predict(x_test))}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 810/1000 [00:23<00:05, 34.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> number of iterations: 810\n",
      ">> loss function on test: 0.27209053103944025\n",
      ">> r2 on test: 0.5678689869729097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNJSqm8duVUj"
   },
   "source": [
    "**Задание 5 (6 балла)**. Постройте графики (на одной и той же картинке) зависимости величины функции потерь от номера итерации для всех реализованных видов стохастического градиентного спусков. Сделайте выводы о скорости сходимости различных модификаций градиентного спуска.\n",
    "\n",
    "Не забывайте о том, что должны получиться *красивые* графики!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6gUTUbdtuVUk"
   },
   "source": [
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1NRgPkeuVUr"
   },
   "source": "**Задание 6 (бонус) (0.01 балла)**.  Вставьте картинку с вашим любимым мемом в этот Jupyter Notebook"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ooLLNAAbw6u9"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework-practice-03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
